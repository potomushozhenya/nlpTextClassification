{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f399c-7627-480d-9955-0992cd117c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "762bc831-0f4d-414a-aea4-3a3e0d0124b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\zhest\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1771\n",
      "1179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from tabulate import tabulate\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "categories = ['rec.autos', 'comp.graphics', 'sci.space']\n",
    "train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, remove=('headers','footer','quotes'))\n",
    "test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, remove=('headers','footer','quotes'))\n",
    "x_train = train.data\n",
    "y_train = train.target\n",
    "x_test = test.data\n",
    "y_test = test.target\n",
    "print(len(x_train))\n",
    "print(len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42eebcf5-8c85-4afe-a2de-87fd7e289c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"\\n\\nCould you use some sort of mechanical chest compression as an aid.\\nSorta like the portable Iron Lung?   Put some sort of flex tubing\\naround the 'aquanauts' chest.  Cyclically compress it  and it will\\npush enough on the chest wall to support breathing?????\\n\\nYou'd have to trust your breather,  but in space, you have to trust\\nyour suit anyway.\\n\\npat\\n\",\n",
       " \"\\n\\nI too would like a 3D graphics library!  How much do C libraries cost\\nanyway?  Can you get the tools used by, say, RenderMan, and can you get\\nthem at a reasonable cost?\\n\\nSorry that I don't have any answers, just questions...\\n\\nMatt Madsen\\nmmadsen@ics.uci.edu\\n\\n\",\n",
       " '\\nFrom: <tom>\\nSubject: computer cult\\n\\nFrom scott Fri Apr 23 16:31:21 1993\\nReceived: by igc.apc.org (4.1/Revision: 1.77 )\\n\\tid AA16121; Fri, 23 Apr 93 16:31:09 PDT\\nDate: Fri, 23 Apr 93 16:31:09 PDT\\nMessage-Id: <9304232331.AA16121@igc.apc.org>\\nFrom: Scott Weikart <scott>\\nSender: scott\\nTo: cdplist\\nSubject: Next stand-off?\\nStatus: R\\n\\nRedwood City, CA (API) -- A tense stand-off entered its third week\\ntoday as authorities reported no progress in negotiations with\\ncharismatic cult leader Steve Jobs.\\n\\nNegotiators are uncertain of the situation inside the compound, but\\nsome reports suggest that half of the hundreds of followers inside\\nhave been terminated.  Others claim to be staying of their own free\\nwill, but Jobs\\' persuasive manner makes this hard to confirm.\\n\\nIn conversations with authorities, Jobs has given conflicting\\ninformation on how heavily prepared the group is for war with the\\nindustry.  At times, he has claimed to \"have hardware which will blow\\nanything else away\", while more recently he claims they have stopped\\nmanufacturing their own.\\n\\nAgents from the ATF (Apple-Taligent Forces) believe that the group is\\nequipped with serious hardware, including 486-caliber pieces and\\npossibly Canon equipment.\\n\\nThe siege has attracted a variety of spectators, from the curious to\\nother cultists.  Some have offered to intercede in negotiations,\\nincluding a young man who will identify himself only as \"Bill\" and\\nclaims to be the \"MS-iah\".\\n\\nFormer members of the cult, some only recently deprogrammed, speak\\nhesitantly of their former lives, including being forced to work\\n20-hour days, and subsisting on Jolt and Twinkies.  There were\\nfrequent lectures in which they were indoctrinated into a theory of\\n\"interpersonal computing\" which rejects traditional roles.\\n\\nLate-night vigils on Chesapeake Drive are taking their toll on\\nfederal marshals.  Loud rock and roll, mostly Talking Heads, blares\\nthroughout the night.  Some fear that Jobs will fulfill his own\\napocalyptic prophecies, a worry reinforced when the loudspeakers\\ncarry Jobs\\' own speeches -- typically beginning with a chilling \"I\\nwant to welcome you to the \\'Next World\\' \".\\n\\n- - -- \\nRoland J. Schemers III              |            Networking Systems\\nSystems Programmer                  |            G16 Redwood Hall (415) 723-6740\\nDistributed Computing Group         |            Stanford, CA 94305-4122\\nStanford University                 |            schemers@Slapshot.Stanford.EDU\\n\\n\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15e6b14e-8973-493d-90af-c3441a198431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4ca4dc9-ec2f-4ab1-9754-e1b736a324aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatizeText(input):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    #Tokenizers divide strings into lists of substrings\n",
    "    wordList = word_tokenize(input)\n",
    "    output = ' '.join([lemmatizer.lemmatize(w) for w in wordList])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "403c7761-f0e2-4a98-8620-c855ed0cb890",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemText(input):\n",
    "    stemmer = PorterStemmer()\n",
    "    wordList = word_tokenize(input)\n",
    "    output = ' '.join([stemmer.stem(w) for w in wordList])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "192cb992-6857-4042-b70a-696f46e8c767",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x_train, x_test, y_train, y_test, n):\n",
    "    clf = DecisionTreeClassifier(random_state=n)\n",
    "    clf.fit(x_train, y_train)\n",
    "    y_pred = clf.predict(x_test)\n",
    "    return f1_score(y_test, y_pred, average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74155e9f-1c77-45da-9c0a-f1a91c806693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textProcessing(unprocessedTexts, function=None):\n",
    "    if function is None:\n",
    "        return unprocessedTexts\n",
    "    return [function(text) for text in unprocessedTexts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d10acd7-978b-411c-b8dc-6de8ab348f4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum: 0.7575131696218897\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=30;m=5      None                Stemming            Lemmatizer\n",
      "One-hot       0.7563519548225734  0.7506792262354128  0.7439125836283446\n",
      "Bag of words  0.7571307245123651  0.7575131696218897  0.7515069047911325\n",
      "Tf-idf        0.729577193823154   0.7396189071321255  0.7446689600034061\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7584707003601143\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=40;m=5      None                Stemming            Lemmatizer\n",
      "One-hot       0.7453063176947534  0.7447810342816077  0.7550361116016518\n",
      "Bag of words  0.7542755529677643  0.7584707003601143  0.7514507206988321\n",
      "Tf-idf        0.7313810969218506  0.7396219265171883  0.7395484918977683\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7556785832828575\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=50;m=5      None                Stemming            Lemmatizer\n",
      "One-hot       0.7500972709894475  0.7524600731579955  0.7523324851569126\n",
      "Bag of words  0.7507689299722943  0.7556785832828575  0.755666127523457\n",
      "Tf-idf        0.7197188688922523  0.7431303757748484  0.7386865145361727\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7660261783367228\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=60;m=5      None                Stemming            Lemmatizer\n",
      "One-hot       0.7617237026657635  0.7534340463255446  0.759952964762125\n",
      "Bag of words  0.7660261783367228  0.7464847689730941  0.759055919687442\n",
      "Tf-idf        0.7146604411576585  0.7363240368064684  0.7336973186039502\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7575131696218897\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=30;m=10     None                Stemming            Lemmatizer\n",
      "One-hot       0.7563519548225734  0.7506792262354128  0.7439125836283446\n",
      "Bag of words  0.7571307245123651  0.7575131696218897  0.7515069047911325\n",
      "Tf-idf        0.729577193823154   0.7396189071321255  0.7446689600034061\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7584707003601143\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=40;m=10     None                Stemming            Lemmatizer\n",
      "One-hot       0.7453063176947534  0.7447810342816077  0.7550361116016518\n",
      "Bag of words  0.7542755529677643  0.7584707003601143  0.7514507206988321\n",
      "Tf-idf        0.7313810969218506  0.7396219265171883  0.7395484918977683\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7556785832828575\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=50;m=10     None                Stemming            Lemmatizer\n",
      "One-hot       0.7500972709894475  0.7524600731579955  0.7523324851569126\n",
      "Bag of words  0.7507689299722943  0.7556785832828575  0.755666127523457\n",
      "Tf-idf        0.7197188688922523  0.7431303757748484  0.7386865145361727\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7660261783367228\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=60;m=10     None                Stemming            Lemmatizer\n",
      "One-hot       0.7617237026657635  0.7534340463255446  0.759952964762125\n",
      "Bag of words  0.7660261783367228  0.7464847689730941  0.759055919687442\n",
      "Tf-idf        0.7146604411576585  0.7363240368064684  0.7336973186039502\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7575131696218897\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=30;m=15     None                Stemming            Lemmatizer\n",
      "One-hot       0.7563519548225734  0.7506792262354128  0.7439125836283446\n",
      "Bag of words  0.7571307245123651  0.7575131696218897  0.7515069047911325\n",
      "Tf-idf        0.729577193823154   0.7396189071321255  0.7446689600034061\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7584707003601143\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=40;m=15     None                Stemming            Lemmatizer\n",
      "One-hot       0.7453063176947534  0.7447810342816077  0.7550361116016518\n",
      "Bag of words  0.7542755529677643  0.7584707003601143  0.7514507206988321\n",
      "Tf-idf        0.7313810969218506  0.7396219265171883  0.7395484918977683\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7556785832828575\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=50;m=15     None                Stemming            Lemmatizer\n",
      "One-hot       0.7500972709894475  0.7524600731579955  0.7523324851569126\n",
      "Bag of words  0.7507689299722943  0.7556785832828575  0.755666127523457\n",
      "Tf-idf        0.7197188688922523  0.7431303757748484  0.7386865145361727\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7660261783367228\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=60;m=15     None                Stemming            Lemmatizer\n",
      "One-hot       0.7617237026657635  0.7534340463255446  0.759952964762125\n",
      "Bag of words  0.7660261783367228  0.7464847689730941  0.759055919687442\n",
      "Tf-idf        0.7146604411576585  0.7363240368064684  0.7336973186039502\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7575131696218897\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=30;m=20     None                Stemming            Lemmatizer\n",
      "One-hot       0.7563519548225734  0.7506792262354128  0.7439125836283446\n",
      "Bag of words  0.7571307245123651  0.7575131696218897  0.7515069047911325\n",
      "Tf-idf        0.729577193823154   0.7396189071321255  0.7446689600034061\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7584707003601143\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=40;m=20     None                Stemming            Lemmatizer\n",
      "One-hot       0.7453063176947534  0.7447810342816077  0.7550361116016518\n",
      "Bag of words  0.7542755529677643  0.7584707003601143  0.7514507206988321\n",
      "Tf-idf        0.7313810969218506  0.7396219265171883  0.7395484918977683\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7556785832828575\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=50;m=20     None                Stemming            Lemmatizer\n",
      "One-hot       0.7500972709894475  0.7524600731579955  0.7523324851569126\n",
      "Bag of words  0.7507689299722943  0.7556785832828575  0.755666127523457\n",
      "Tf-idf        0.7197188688922523  0.7431303757748484  0.7386865145361727\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7660261783367228\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=60;m=20     None                Stemming            Lemmatizer\n",
      "One-hot       0.7617237026657635  0.7534340463255446  0.759952964762125\n",
      "Bag of words  0.7660261783367228  0.7464847689730941  0.759055919687442\n",
      "Tf-idf        0.7146604411576585  0.7363240368064684  0.7336973186039502\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7575131696218897\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=30;m=25     None                Stemming            Lemmatizer\n",
      "One-hot       0.7563519548225734  0.7506792262354128  0.7439125836283446\n",
      "Bag of words  0.7571307245123651  0.7575131696218897  0.7515069047911325\n",
      "Tf-idf        0.729577193823154   0.7396189071321255  0.7446689600034061\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7584707003601143\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=40;m=25     None                Stemming            Lemmatizer\n",
      "One-hot       0.7453063176947534  0.7447810342816077  0.7550361116016518\n",
      "Bag of words  0.7542755529677643  0.7584707003601143  0.7514507206988321\n",
      "Tf-idf        0.7313810969218506  0.7396219265171883  0.7395484918977683\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7556785832828575\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=50;m=25     None                Stemming            Lemmatizer\n",
      "One-hot       0.7500972709894475  0.7524600731579955  0.7523324851569126\n",
      "Bag of words  0.7507689299722943  0.7556785832828575  0.755666127523457\n",
      "Tf-idf        0.7197188688922523  0.7431303757748484  0.7386865145361727\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7660261783367228\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=60;m=25     None                Stemming            Lemmatizer\n",
      "One-hot       0.7617237026657635  0.7534340463255446  0.759952964762125\n",
      "Bag of words  0.7660261783367228  0.7464847689730941  0.759055919687442\n",
      "Tf-idf        0.7146604411576585  0.7363240368064684  0.7336973186039502\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7575131696218897\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=30;m=30     None                Stemming            Lemmatizer\n",
      "One-hot       0.7563519548225734  0.7506792262354128  0.7439125836283446\n",
      "Bag of words  0.7571307245123651  0.7575131696218897  0.7515069047911325\n",
      "Tf-idf        0.729577193823154   0.7396189071321255  0.7446689600034061\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7584707003601143\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=40;m=30     None                Stemming            Lemmatizer\n",
      "One-hot       0.7453063176947534  0.7447810342816077  0.7550361116016518\n",
      "Bag of words  0.7542755529677643  0.7584707003601143  0.7514507206988321\n",
      "Tf-idf        0.7313810969218506  0.7396219265171883  0.7395484918977683\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7556785832828575\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=50;m=30     None                Stemming            Lemmatizer\n",
      "One-hot       0.7500972709894475  0.7524600731579955  0.7523324851569126\n",
      "Bag of words  0.7507689299722943  0.7556785832828575  0.755666127523457\n",
      "Tf-idf        0.7197188688922523  0.7431303757748484  0.7386865145361727\n",
      "------------  ------------------  ------------------  ------------------\n",
      "Maximum: 0.7660261783367228\n",
      "------------  ------------------  ------------------  ------------------\n",
      "n=60;m=30     None                Stemming            Lemmatizer\n",
      "One-hot       0.7617237026657635  0.7534340463255446  0.759952964762125\n",
      "Bag of words  0.7660261783367228  0.7464847689730941  0.759055919687442\n",
      "Tf-idf        0.7146604411576585  0.7363240368064684  0.7336973186039502\n",
      "------------  ------------------  ------------------  ------------------\n"
     ]
    }
   ],
   "source": [
    "for m in [5,10,15,20,25,30]:\n",
    "    for n in [30,40,50,60]:\n",
    "        tableText = [[\"\" for j in range(4)] for i in range(4)]\n",
    "        tableText[0] = [f\"n={n};m={m}\", \"None\", \"Stemming\", \"Lemmatizer\"]\n",
    "        tableText[1][0] = \"One-hot\"\n",
    "        tableText[2][0] = \"Bag of words\"\n",
    "        tableText[3][0] = \"Tf-idf\"\n",
    "        for i in range(3):\n",
    "            if i==0:\n",
    "                func = None\n",
    "            if i==1:\n",
    "                func = stemText\n",
    "            if i==2:\n",
    "                func = lemmatizeText\n",
    "            preprocessedTrain = textProcessing(x_train, func)\n",
    "            preprocessedTest = textProcessing(x_test, func)\n",
    "            for j in range(3):\n",
    "                if j==0:\n",
    "                    vect = CountVectorizer(binary=True, stop_words=stop_words)\n",
    "                if j==1:\n",
    "                    vect = CountVectorizer(binary=False, stop_words=stop_words)\n",
    "                if j==2:\n",
    "                    vect = TfidfVectorizer(stop_words=stop_words)\n",
    "                train_vec = vect.fit_transform(preprocessedTrain)\n",
    "                test_vec = vect.transform(preprocessedTest)\n",
    "    \n",
    "                f1 = train(train_vec, test_vec, y_train, y_test, n)\n",
    "                tableText[j+1][i+1] = f1\n",
    "        list = [max(tableText[i][1:]) for i in range(1,4)]\n",
    "        maximum = max(list)\n",
    "        print(f\"Maximum: {maximum}\")\n",
    "        print(tabulate(tableText))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a38ce1-8a4b-40e8-9d13-537ede85bcac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
